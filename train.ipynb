{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6d68f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: psutil in c:\\users\\abhis\\appdata\\roaming\\python\\python310\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/10.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/10.4 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/10.4 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.4 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.6/10.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl (75.4 MB)\n",
      "   ---------------------------------------- 0.0/75.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/75.4 MB 5.6 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 2.1/75.4 MB 5.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 3.4/75.4 MB 5.4 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 4.5/75.4 MB 5.3 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 5.2/75.4 MB 5.1 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 5.8/75.4 MB 5.0 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 7.1/75.4 MB 4.9 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 8.1/75.4 MB 4.8 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 9.2/75.4 MB 4.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 10.2/75.4 MB 4.9 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 11.3/75.4 MB 4.8 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 11.8/75.4 MB 4.8 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 13.4/75.4 MB 4.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 14.4/75.4 MB 4.9 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 15.5/75.4 MB 4.9 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 16.8/75.4 MB 5.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 17.8/75.4 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 19.1/75.4 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 20.2/75.4 MB 5.0 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 21.5/75.4 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 22.8/75.4 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 24.1/75.4 MB 5.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 24.9/75.4 MB 5.1 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 25.7/75.4 MB 5.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 27.0/75.4 MB 5.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 28.0/75.4 MB 5.1 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 29.1/75.4 MB 5.1 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 30.4/75.4 MB 5.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 31.2/75.4 MB 5.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 32.2/75.4 MB 5.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 33.6/75.4 MB 5.1 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 34.6/75.4 MB 5.1 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 35.9/75.4 MB 5.1 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 37.0/75.4 MB 5.1 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 38.0/75.4 MB 5.1 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 39.3/75.4 MB 5.1 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 40.4/75.4 MB 5.1 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 41.4/75.4 MB 5.1 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 42.7/75.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 44.0/75.4 MB 5.2 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 45.4/75.4 MB 5.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 46.4/75.4 MB 5.2 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 47.7/75.4 MB 5.2 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 49.3/75.4 MB 5.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 50.9/75.4 MB 5.3 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 51.9/75.4 MB 5.3 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 53.0/75.4 MB 5.3 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 54.3/75.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 55.1/75.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 56.4/75.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 57.4/75.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 58.5/75.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 59.8/75.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 60.8/75.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 61.3/75.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 61.9/75.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 63.4/75.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 64.2/75.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 65.0/75.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 65.8/75.4 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 66.8/75.4 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 67.6/75.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 68.7/75.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 69.7/75.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 70.8/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 71.3/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 72.4/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 73.4/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  74.4/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.4/75.4 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.3 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.1/25.3 MB 5.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.1/25.3 MB 5.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.5/25.3 MB 5.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/25.3 MB 5.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 6.0/25.3 MB 5.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.9/25.3 MB 4.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/25.3 MB 4.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 10.0/25.3 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.3/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.1/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.4/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.7/25.3 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.0/25.3 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.4/25.3 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.4/25.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.0/25.3 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.1/25.3 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.9/25.3 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow, fsspec, dill, multiprocess, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, peft, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.3\n",
      "    Uninstalling huggingface-hub-0.29.3:\n",
      "      Successfully uninstalled huggingface-hub-0.29.3\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 huggingface-hub-0.30.2 multiprocess-0.70.16 peft-0.15.2 pyarrow-19.0.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9d5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0e44c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 145173.22 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 416556.16 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 425832.36 examples/s]\n",
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4537.84 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:06<00:00, 3879.28 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:19<00:00, 2505.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "encoded = dataset.map(tokenize_fn, batched=True)\n",
    "encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32c3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff79e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # for DistilBERT\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918d7cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-distilbert\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"].shuffle(seed=42).select(range(2000)),  # Use small subset\n",
    "    eval_dataset=encoded[\"test\"].select(range(1000)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9635035e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 29:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4718281230926514, metrics={'train_runtime': 1797.8254, 'train_samples_per_second': 2.225, 'train_steps_per_second': 0.278, 'total_flos': 269478813696000.0, 'train_loss': 0.4718281230926514, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cf5585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora-distilbert-sentiment\\\\tokenizer_config.json',\n",
       " './lora-distilbert-sentiment\\\\special_tokens_map.json',\n",
       " './lora-distilbert-sentiment\\\\vocab.txt',\n",
       " './lora-distilbert-sentiment\\\\added_tokens.json',\n",
       " './lora-distilbert-sentiment\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./lora-distilbert-sentiment\")\n",
    "tokenizer.save_pretrained(\"./lora-distilbert-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e3fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie, it was amazing!\n",
      "Predicted Class: 1\n",
      "--------------------------------------------------\n",
      "Text: I hated the food, it was terrible.\n",
      "Predicted Class: 0\n",
      "--------------------------------------------------\n",
      "Text: The product quality is fantastic!\n",
      "Predicted Class: 1\n",
      "--------------------------------------------------\n",
      "Text: Worst service ever, I'm very disappointed.\n",
      "Predicted Class: 0\n",
      "--------------------------------------------------\n",
      "Text: I,m dead\n",
      "Predicted Class: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# Step 1: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./lora-distilbert-sentiment\")\n",
    "\n",
    "# Step 2: Load base model and apply LoRA adapters\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora-distilbert-sentiment\")\n",
    "\n",
    "# Step 3: Define a prediction function\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax(dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Step 4: Test it on new sentences!\n",
    "test_sentences = [\n",
    "    \"I love this movie, it was amazing!\",\n",
    "    \"I hated the food, it was terrible.\",\n",
    "    \"The product quality is fantastic!\",\n",
    "    \"Worst service ever, I'm very disappointed.\",\n",
    "    \"I,m dead\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    prediction = predict(sentence)\n",
    "    print(f\"Text: {sentence}\")\n",
    "    print(f\"Predicted Class: {prediction}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9ce10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
